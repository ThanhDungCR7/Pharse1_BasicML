{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "027049a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bf0fdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a13cbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed=2022):\n",
    "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # multi-GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24c49115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Set seeds for reproducibility\n",
    "set_seeds(seed=SEED)\n",
    "# Set device\n",
    "cuda = True\n",
    "device = torch.device(\"cuda\" if (\n",
    "    torch.cuda.is_available() and cuda) else \"cpu\")\n",
    "torch.set_default_tensor_type(\"torch.FloatTensor\")\n",
    "if device.type == \"cuda\":\n",
    "    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "print (device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7236cd",
   "metadata": {},
   "source": [
    "## Tải dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1aba4f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bowden Begins Work</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ganguly to captain Asia in tsunami fundraising...</td>\n",
       "      <td>World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Record for personal bankruptcies</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NetSuite relaunches, Oracle delays small busin...</td>\n",
       "      <td>Sci/Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Greek Sprinters Given Two-Day Reprieve</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  category\n",
       "0                                 Bowden Begins Work    Sports\n",
       "1  Ganguly to captain Asia in tsunami fundraising...     World\n",
       "2                   Record for personal bankruptcies  Business\n",
       "3  NetSuite relaunches, Oracle delays small busin...  Sci/Tech\n",
       "4             Greek Sprinters Given Two-Day Reprieve    Sports"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "url = \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/news.csv\"\n",
    "df = pd.read_csv(url, header=0) # load\n",
    "df = df.sample(frac=1).reset_index(drop=True) # shuffle\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7915b30",
   "metadata": {},
   "source": [
    "## Tiền xử lý"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfa16f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eeb8c50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "STOPWORDS = stopwords.words(\"english\")\n",
    "print (STOPWORDS[:10])\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "667557f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, stopwords=STOPWORDS):\n",
    "    \"\"\"Conditional preprocessing on our text unique to our task.\"\"\"\n",
    "    # Lower\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove stopwords\n",
    "    pattern = re.compile(r\"\\b(\" + r\"|\".join(stopwords) + r\")\\b\\s*\")\n",
    "    text = pattern.sub(\"\", text)\n",
    "\n",
    "    # Remove words in parenthesis\n",
    "    text = re.sub(r\"\\([^)]*\\)\", \"\", text)\n",
    "\n",
    "    # Spacing and filters\n",
    "    text = re.sub(r\"([-;;.,!?<=>])\", r\" \\1 \", text)\n",
    "    text = re.sub(\"[^A-Za-z0-9]+\", \" \", text) # remove non alphanumeric chars\n",
    "    text = re.sub(\" +\", \" \", text)  # remove multiple spaces\n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2a95b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good morning sir'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample\n",
    "text = \"GooD morning SIR!\"\n",
    "preprocess(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a072c98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bowden Begins Work\n",
      "\n",
      "bowden begins work\n"
     ]
    }
   ],
   "source": [
    "# Apply to dataframe\n",
    "preprocessed_df = df.copy()\n",
    "preprocessed_df.title = preprocessed_df.title.apply(preprocess)\n",
    "print (f\"{df.title.values[0]}\\n\\n{preprocessed_df.title.values[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13514273",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d893a0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0d0d3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.7\n",
    "VAL_SIZE = 0.15\n",
    "TEST_SIZE = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddceaffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(X, y, train_size):\n",
    "    \"\"\"Split dataset into data splits.\"\"\"\n",
    "    X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE, stratify=y)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_, y_, train_size=0.5, stratify=y_)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a03cd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "X = preprocessed_df[\"title\"].values\n",
    "y = preprocessed_df[\"category\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "520dab63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (84000,), y_train: (84000,)\n",
      "X_val: (18000,), y_val: (18000,)\n",
      "X_test: (18000,), y_test: (18000,)\n",
      "Sample point: white sox deal lee brewers podsednik → Sports\n"
     ]
    }
   ],
   "source": [
    "# Create data splits\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(\n",
    "    X=X, y=y, train_size=TRAIN_SIZE)\n",
    "print (f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print (f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "print (f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "print (f\"Sample point: {X_train[0]} → {y_train[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6d8fd2",
   "metadata": {},
   "source": [
    "## Đưa Label và word về dạng số"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2211f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0b30f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoder(object):\n",
    "    \"\"\"Label encoder for tag labels.\"\"\"\n",
    "    def __init__(self, class_to_index={}):\n",
    "        self.class_to_index = class_to_index or {}  # mutable defaults ;)\n",
    "        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\n",
    "        self.classes = list(self.class_to_index.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.class_to_index)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<LabelEncoder(num_classes={len(self)})>\"\n",
    "\n",
    "    def fit(self, y):\n",
    "        classes = np.unique(y)\n",
    "        for i, class_ in enumerate(classes):\n",
    "            self.class_to_index[class_] = i\n",
    "        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\n",
    "        self.classes = list(self.class_to_index.keys())\n",
    "        return self\n",
    "\n",
    "    def encode(self, y):\n",
    "        encoded = np.zeros((len(y)), dtype=int)\n",
    "        for i, item in enumerate(y):\n",
    "            encoded[i] = self.class_to_index[item]\n",
    "        return encoded\n",
    "\n",
    "    def decode(self, y):\n",
    "        classes = []\n",
    "        for i, item in enumerate(y):\n",
    "            classes.append(self.index_to_class[item])\n",
    "        return classes\n",
    "\n",
    "    def save(self, fp):\n",
    "        with open(fp, \"w\") as fp:\n",
    "            contents = {'class_to_index': self.class_to_index}\n",
    "            json.dump(contents, fp, indent=4, sort_keys=False)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, fp):\n",
    "        with open(fp, \"r\") as fp:\n",
    "            kwargs = json.load(fp=fp)\n",
    "        return cls(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdbfdff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Business': 0, 'Sci/Tech': 1, 'Sports': 2, 'World': 3}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "NUM_CLASSES = len(label_encoder)\n",
    "label_encoder.class_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4e4719d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train[0]: Sports\n",
      "y_train[0]: 2\n"
     ]
    }
   ],
   "source": [
    "# Convert labels to tokens\n",
    "print (f\"y_train[0]: {y_train[0]}\")\n",
    "y_train = label_encoder.encode(y_train)\n",
    "y_val = label_encoder.encode(y_val)\n",
    "y_test = label_encoder.encode(y_test)\n",
    "print (f\"y_train[0]: {y_train[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73e48841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts: [21000 21000 21000 21000]\n",
      "weights: {0: 4.761904761904762e-05, 1: 4.761904761904762e-05, 2: 4.761904761904762e-05, 3: 4.761904761904762e-05}\n"
     ]
    }
   ],
   "source": [
    "# Class weights\n",
    "counts = np.bincount(y_train)\n",
    "class_weights = {i: 1.0/count for i, count in enumerate(counts)}\n",
    "print (f\"counts: {counts}\\nweights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ead6951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from more_itertools import take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8a57141",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    def __init__(self, char_level, num_tokens=None,\n",
    "                 pad_token=\"<PAD>\", oov_token=\"<UNK>\",\n",
    "                 token_to_index=None):\n",
    "        self.char_level = char_level\n",
    "        self.separator = \"\" if self.char_level else \" \"\n",
    "        if num_tokens: num_tokens -= 2 # pad + unk tokens\n",
    "        self.num_tokens = num_tokens\n",
    "        self.pad_token = pad_token\n",
    "        self.oov_token = oov_token\n",
    "        if not token_to_index:\n",
    "            token_to_index = {pad_token: 0, oov_token: 1}\n",
    "        self.token_to_index = token_to_index\n",
    "        self.index_to_token = {v: k for k, v in self.token_to_index.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_index)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<Tokenizer(num_tokens={len(self)})>\"\n",
    "\n",
    "    def fit_on_texts(self, texts):\n",
    "        if not self.char_level:\n",
    "            texts = [text.split(\" \") for text in texts]\n",
    "        all_tokens = [token for text in texts for token in text]\n",
    "        counts = Counter(all_tokens).most_common(self.num_tokens)\n",
    "        self.min_token_freq = counts[-1][1]\n",
    "        for token, count in counts:\n",
    "            index = len(self)\n",
    "            self.token_to_index[token] = index\n",
    "            self.index_to_token[index] = token\n",
    "        return self\n",
    "\n",
    "    def texts_to_sequences(self, texts):\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            if not self.char_level:\n",
    "                text = text.split(\" \")\n",
    "            sequence = []\n",
    "            for token in text:\n",
    "                sequence.append(self.token_to_index.get(\n",
    "                    token, self.token_to_index[self.oov_token]))\n",
    "            sequences.append(np.asarray(sequence))\n",
    "        return sequences\n",
    "\n",
    "    def sequences_to_texts(self, sequences):\n",
    "        texts = []\n",
    "        for sequence in sequences:\n",
    "            text = []\n",
    "            for index in sequence:\n",
    "                text.append(self.index_to_token.get(index, self.oov_token))\n",
    "            texts.append(self.separator.join([token for token in text]))\n",
    "        return texts\n",
    "\n",
    "    def save(self, fp):\n",
    "        with open(fp, \"w\") as fp:\n",
    "            contents = {\n",
    "                \"char_level\": self.char_level,\n",
    "                \"oov_token\": self.oov_token,\n",
    "                \"token_to_index\": self.token_to_index\n",
    "            }\n",
    "            json.dump(contents, fp, indent=4, sort_keys=False)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, fp):\n",
    "        with open(fp, \"r\") as fp:\n",
    "            kwargs = json.load(fp=fp)\n",
    "        return cls(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e379f33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Tokenizer(num_tokens=5000)>\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "tokenizer = Tokenizer(char_level=False, num_tokens=5000)\n",
    "tokenizer.fit_on_texts(texts=X_train)\n",
    "VOCAB_SIZE = len(tokenizer)\n",
    "print (tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c69e756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<PAD>', 0), ('<UNK>', 1), ('39', 2), ('b', 3), ('gt', 4)]\n",
      "least freq token's freq: 14\n"
     ]
    }
   ],
   "source": [
    "# Sample of tokens\n",
    "print (take(5, tokenizer.token_to_index.items()))\n",
    "print (f\"least freq token's freq: {tokenizer.min_token_freq}\") # use this to adjust num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22613a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text to indices:\n",
      "  (preprocessed) → white sox deal lee brewers <UNK>\n",
      "  (tokenized) → [ 602  103   15 2511 2992    1]\n"
     ]
    }
   ],
   "source": [
    "# Convert texts to sequences of indices\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "preprocessed_text = tokenizer.sequences_to_texts([X_train[0]])[0]\n",
    "print (\"Text to indices:\\n\"\n",
    "    f\"  (preprocessed) → {preprocessed_text}\\n\"\n",
    "    f\"  (tokenized) → {X_train[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a782b440",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76c5c5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, max_seq_len=0):\n",
    "    \"\"\"Pad sequences to max length in sequence.\"\"\"\n",
    "    max_seq_len = max(max_seq_len, max(len(sequence) for sequence in sequences))\n",
    "    padded_sequences = np.zeros((len(sequences), max_seq_len))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        padded_sequences[i][:len(sequence)] = sequence\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8596dc44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,  352,   93, 1327,    1,   35,  245])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "689db4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 7)\n",
      "[[6.020e+02 1.030e+02 1.500e+01 2.511e+03 2.992e+03 1.000e+00 0.000e+00]\n",
      " [1.000e+00 3.520e+02 9.300e+01 1.327e+03 1.000e+00 3.500e+01 2.450e+02]\n",
      " [1.578e+03 4.510e+03 2.370e+02 7.310e+02 1.603e+03 3.465e+03 0.000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# 2D sequences\n",
    "padded = pad_sequences(X_train[0:3])\n",
    "print (padded.shape)\n",
    "print (padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70636847",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f616d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<Dataset(N={len(self)})>\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.X[index]\n",
    "        y = self.y[index]\n",
    "        return [X, len(X), y]\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"Processing on a batch.\"\"\"\n",
    "        # Get inputs\n",
    "        batch = np.array(batch)\n",
    "        X = batch[:, 0]\n",
    "        seq_lens = batch[:, 1]\n",
    "        y = batch[:, 2]\n",
    "\n",
    "        # Pad inputs\n",
    "        X = pad_sequences(sequences=X)\n",
    "\n",
    "        # Cast\n",
    "        X = torch.LongTensor(X.astype(np.int32))\n",
    "        seq_lens = torch.LongTensor(seq_lens.astype(np.int32))\n",
    "        y = torch.LongTensor(y.astype(np.int32))\n",
    "\n",
    "        return X, seq_lens, y\n",
    "\n",
    "    def create_dataloader(self, batch_size, shuffle=False, drop_last=False):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset=self, batch_size=batch_size, collate_fn=self.collate_fn,\n",
    "            shuffle=shuffle, drop_last=drop_last, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a3ce52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets:\n",
      "  Train dataset:<Dataset(N=84000)>\n",
      "  Val dataset: <Dataset(N=18000)>\n",
      "  Test dataset: <Dataset(N=18000)>\n",
      "Sample point:\n",
      "  X: [ 602  103   15 2511 2992    1]\n",
      "  seq_len: 6\n",
      "  y: 2\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_dataset = Dataset(X=X_train, y=y_train)\n",
    "val_dataset = Dataset(X=X_val, y=y_val)\n",
    "test_dataset = Dataset(X=X_test, y=y_test)\n",
    "print (\"Datasets:\\n\"\n",
    "    f\"  Train dataset:{train_dataset.__str__()}\\n\"\n",
    "    f\"  Val dataset: {val_dataset.__str__()}\\n\"\n",
    "    f\"  Test dataset: {test_dataset.__str__()}\\n\"\n",
    "    \"Sample point:\\n\"\n",
    "    f\"  X: {train_dataset[0][0]}\\n\"\n",
    "    f\"  seq_len: {train_dataset[0][1]}\\n\"\n",
    "    f\"  y: {train_dataset[0][2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2b9e3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch:\n",
      "  X: [64, 13]\n",
      "  seq_lens: [64]\n",
      "  y: [64]\n",
      "Sample point:\n",
      "  X: tensor([ 602,  103,   15, 2511, 2992,    1,    0,    0,    0,    0,    0,    0,\n",
      "           0])\n",
      " seq_len: 6\n",
      "  y: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp/ipykernel_16312/906435048.py:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  batch = np.array(batch)\n"
     ]
    }
   ],
   "source": [
    "# Create dataloaders\n",
    "batch_size = 64\n",
    "train_dataloader = train_dataset.create_dataloader(\n",
    "    batch_size=batch_size)\n",
    "val_dataloader = val_dataset.create_dataloader(\n",
    "    batch_size=batch_size)\n",
    "test_dataloader = test_dataset.create_dataloader(\n",
    "    batch_size=batch_size)\n",
    "batch_X, batch_seq_lens, batch_y = next(iter(train_dataloader))\n",
    "print (\"Sample batch:\\n\"\n",
    "    f\"  X: {list(batch_X.size())}\\n\"\n",
    "    f\"  seq_lens: {list(batch_seq_lens.size())}\\n\"\n",
    "    f\"  y: {list(batch_y.size())}\\n\"\n",
    "    \"Sample point:\\n\"\n",
    "    f\"  X: {batch_X[0]}\\n\"\n",
    "    f\" seq_len: {batch_seq_lens[0]}\\n\"\n",
    "    f\"  y: {batch_y[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b160af9",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a3bd376",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, model, device, loss_fn=None, optimizer=None, scheduler=None):\n",
    "\n",
    "        # Set params\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "    def train_step(self, dataloader):\n",
    "        \"\"\"Train step.\"\"\"\n",
    "        # Set model to train mode\n",
    "        self.model.train()\n",
    "        loss = 0.0\n",
    "\n",
    "        # Iterate over train batches\n",
    "        for i, batch in enumerate(dataloader):\n",
    "\n",
    "            # Step\n",
    "            batch = [item.to(self.device) for item in batch]  # Set device\n",
    "            inputs, targets = batch[:-1], batch[-1]\n",
    "            self.optimizer.zero_grad()  # Reset gradients\n",
    "            z = self.model(inputs)  # Forward pass\n",
    "            J = self.loss_fn(z, targets)  # Define loss\n",
    "            J.backward()  # Backward pass\n",
    "            self.optimizer.step()  # Update weights\n",
    "\n",
    "            # Cumulative Metrics\n",
    "            loss += (J.detach().item() - loss) / (i + 1)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def eval_step(self, dataloader):\n",
    "        \"\"\"Validation or test step.\"\"\"\n",
    "        # Set model to eval mode\n",
    "        self.model.eval()\n",
    "        loss = 0.0\n",
    "        y_trues, y_probs = [], []\n",
    "\n",
    "        # Iterate over val batches\n",
    "        with torch.inference_mode():\n",
    "            for i, batch in enumerate(dataloader):\n",
    "\n",
    "                # Step\n",
    "                batch = [item.to(self.device) for item in batch]  # Set device\n",
    "                inputs, y_true = batch[:-1], batch[-1]\n",
    "                z = self.model(inputs)  # Forward pass\n",
    "                J = self.loss_fn(z, y_true).item()\n",
    "\n",
    "                # Cumulative Metrics\n",
    "                loss += (J - loss) / (i + 1)\n",
    "\n",
    "                # Store outputs\n",
    "                y_prob = torch.sigmoid(z).cpu().numpy()\n",
    "                y_probs.extend(y_prob)\n",
    "                y_trues.extend(y_true.cpu().numpy())\n",
    "\n",
    "        return loss, np.vstack(y_trues), np.vstack(y_probs)\n",
    "\n",
    "    def predict_step(self, dataloader):\n",
    "        \"\"\"Prediction step.\"\"\"\n",
    "        # Set model to eval mode\n",
    "        self.model.eval()\n",
    "        y_probs = []\n",
    "\n",
    "        # Iterate over val batches\n",
    "        with torch.inference_mode():\n",
    "            for i, batch in enumerate(dataloader):\n",
    "\n",
    "                # Forward pass w/ inputs\n",
    "                inputs, targets = batch[:-1], batch[-1]\n",
    "                y_prob = F.softmax(model(inputs), dim=1)\n",
    "\n",
    "                # Store outputs\n",
    "                y_probs.extend(y_prob)\n",
    "\n",
    "        return np.vstack(y_probs)\n",
    "\n",
    "    def train(self, num_epochs, patience, train_dataloader, val_dataloader):\n",
    "        best_val_loss = np.inf\n",
    "        for epoch in range(num_epochs):\n",
    "            # Steps\n",
    "            train_loss = self.train_step(dataloader=train_dataloader)\n",
    "            val_loss, _, _ = self.eval_step(dataloader=val_dataloader)\n",
    "            self.scheduler.step(val_loss)\n",
    "\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = self.model\n",
    "                _patience = patience  # reset _patience\n",
    "            else:\n",
    "                _patience -= 1\n",
    "            if not _patience:  # 0\n",
    "                print(\"Stopping early!\")\n",
    "                break\n",
    "\n",
    "            # Logging\n",
    "            print(\n",
    "                f\"Epoch: {epoch+1} | \"\n",
    "                f\"train_loss: {train_loss:.5f}, \"\n",
    "                f\"val_loss: {val_loss:.5f}, \"\n",
    "                f\"lr: {self.optimizer.param_groups[0]['lr']:.2E}, \"\n",
    "                f\"_patience: {_patience}\"\n",
    "            )\n",
    "        return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78e2c76",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ba0f8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "73f3ae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "SEQ_LEN = 8\n",
    "EMBEDDING_DIM = 100\n",
    "RNN_HIDDEN_DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d3c6901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed\n",
    "x = torch.rand((BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f40858e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out:  torch.Size([64, 8, 128])\n",
      "h_n:  torch.Size([1, 64, 128])\n"
     ]
    }
   ],
   "source": [
    "# Encode\n",
    "rnn = nn.RNN(EMBEDDING_DIM, RNN_HIDDEN_DIM, batch_first=True)\n",
    "out, h_n = rnn(x) # h_n is the last hidden state\n",
    "print (\"out: \", out.shape)\n",
    "print (\"h_n: \", h_n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3fc8bc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:  torch.Size([64, 8, 1])\n",
      "attn_vals:  torch.Size([64, 8])\n",
      "attn_vals[0]:  tensor([0.1455, 0.1122, 0.1204, 0.1240, 0.1407, 0.1255, 0.1219, 0.1098],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "sum(attn_vals[0]):  tensor(1.0000, grad_fn=<AddBackward0>)\n",
      "c:  torch.Size([64, 128])\n"
     ]
    }
   ],
   "source": [
    "# Attend\n",
    "attn = nn.Linear(RNN_HIDDEN_DIM, 1)\n",
    "e = attn(out)\n",
    "attn_vals = F.softmax(e.squeeze(2), dim=1)\n",
    "c = torch.bmm(attn_vals.unsqueeze(1), out).squeeze(1)\n",
    "print (\"e: \", e.shape)\n",
    "print (\"attn_vals: \", attn_vals.shape)\n",
    "print (\"attn_vals[0]: \", attn_vals[0])\n",
    "print (\"sum(attn_vals[0]): \", sum(attn_vals[0]))\n",
    "print (\"c: \", c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b6093b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:  torch.Size([64, 4])\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "fc1 = nn.Linear(RNN_HIDDEN_DIM, NUM_CLASSES)\n",
    "output = F.softmax(fc1(c), dim=1)\n",
    "print (\"output: \", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa66d286",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8361d457",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_HIDDEN_DIM = 128\n",
    "DROPOUT_P = 0.1\n",
    "HIDDEN_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0fcfe249",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size, rnn_hidden_dim,\n",
    "                 hidden_dim, dropout_p, num_classes, padding_idx=0):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        # Initialize embeddings\n",
    "        self.embeddings = nn.Embedding(\n",
    "            embedding_dim=embedding_dim, num_embeddings=vocab_size,\n",
    "            padding_idx=padding_idx)\n",
    "\n",
    "        # RNN\n",
    "        self.rnn = nn.RNN(embedding_dim, rnn_hidden_dim, batch_first=True)\n",
    "\n",
    "        # Attention\n",
    "        self.attn = nn.Linear(rnn_hidden_dim, 1)\n",
    "\n",
    "        # FC weights\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Embed\n",
    "        x_in, seq_lens = inputs\n",
    "        x_in = self.embeddings(x_in)\n",
    "\n",
    "        # Encode\n",
    "        out, h_n = self.rnn(x_in)\n",
    "\n",
    "        # Attend\n",
    "        e = self.attn(out)\n",
    "        attn_vals = F.softmax(e.squeeze(2), dim=1)\n",
    "        c = torch.bmm(attn_vals.unsqueeze(1), out).squeeze(1)\n",
    "\n",
    "        # Predict\n",
    "        z = self.fc1(c)\n",
    "        z = self.dropout(z)\n",
    "        z = self.fc2(z)\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "021dc278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.named_parameters of RNN(\n",
      "  (embeddings): Embedding(5000, 100, padding_idx=0)\n",
      "  (rnn): RNN(100, 128, batch_first=True)\n",
      "  (attn): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (fc1): Linear(in_features=128, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=4, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "# Simple RNN cell\n",
    "model = RNN(\n",
    "    embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE,\n",
    "    rnn_hidden_dim=RNN_HIDDEN_DIM, hidden_dim=HIDDEN_DIM,\n",
    "    dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)\n",
    "model = model.to(device) # set device\n",
    "print (model.named_parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e783eb0c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "08b91ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "NUM_LAYERS = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "PATIENCE = 10\n",
    "NUM_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5a3e16b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss\n",
    "class_weights_tensor = torch.Tensor(list(class_weights.values())).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "933f3167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer & scheduler\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.1, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7e61f5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp/ipykernel_16312/906435048.py:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  batch = np.array(batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.20933, val_loss: 1.07856, lr: 1.00E-04, _patience: 10\n",
      "Epoch: 2 | train_loss: 0.99495, val_loss: 0.93221, lr: 1.00E-04, _patience: 10\n",
      "Epoch: 3 | train_loss: 0.86709, val_loss: 0.83007, lr: 1.00E-04, _patience: 10\n",
      "Epoch: 4 | train_loss: 0.77743, val_loss: 0.75916, lr: 1.00E-04, _patience: 10\n",
      "Epoch: 5 | train_loss: 0.71190, val_loss: 0.70862, lr: 1.00E-04, _patience: 10\n",
      "Epoch: 6 | train_loss: 0.66176, val_loss: 0.67016, lr: 1.00E-04, _patience: 10\n",
      "Epoch: 7 | train_loss: 0.62107, val_loss: 0.63985, lr: 1.00E-04, _patience: 10\n",
      "Epoch: 8 | train_loss: 0.58754, val_loss: 0.61600, lr: 1.00E-04, _patience: 10\n",
      "Epoch: 9 | train_loss: 0.55998, val_loss: 0.59743, lr: 1.00E-04, _patience: 10\n",
      "Epoch: 10 | train_loss: 0.53642, val_loss: 0.58221, lr: 1.00E-04, _patience: 10\n",
      "Epoch: 11 | train_loss: 0.51581, val_loss: 0.56978, lr: 1.00E-04, _patience: 10\n",
      "Epoch: 12 | train_loss: 0.49797, val_loss: 0.55965, lr: 1.00E-04, _patience: 10\n",
      "Epoch: 13 | train_loss: 0.48228, val_loss: 0.55076, lr: 1.00E-04, _patience: 10\n",
      "Epoch: 14 | train_loss: 0.46805, val_loss: 0.54476, lr: 1.00E-04, _patience: 10\n",
      "Epoch: 15 | train_loss: 0.45498, val_loss: 0.53902, lr: 1.00E-04, _patience: 10\n",
      "Epoch: 16 | train_loss: 0.44332, val_loss: 0.53558, lr: 1.00E-04, _patience: 10\n",
      "Epoch: 17 | train_loss: 0.43180, val_loss: 0.53203, lr: 1.00E-04, _patience: 10\n",
      "Epoch: 18 | train_loss: 0.42177, val_loss: 0.52913, lr: 1.00E-04, _patience: 10\n",
      "Epoch: 19 | train_loss: 0.41223, val_loss: 0.52859, lr: 1.00E-04, _patience: 10\n",
      "Epoch: 20 | train_loss: 0.40337, val_loss: 0.52893, lr: 1.00E-04, _patience: 9\n",
      "Epoch: 21 | train_loss: 0.39452, val_loss: 0.52892, lr: 1.00E-04, _patience: 8\n",
      "Epoch: 22 | train_loss: 0.38629, val_loss: 0.52804, lr: 1.00E-04, _patience: 10\n",
      "Epoch: 23 | train_loss: 0.37786, val_loss: 0.53055, lr: 1.00E-04, _patience: 9\n",
      "Epoch: 24 | train_loss: 0.37016, val_loss: 0.53185, lr: 1.00E-04, _patience: 8\n",
      "Epoch: 25 | train_loss: 0.36291, val_loss: 0.53301, lr: 1.00E-04, _patience: 7\n",
      "Epoch: 26 | train_loss: 0.35538, val_loss: 0.53778, lr: 1.00E-05, _patience: 6\n",
      "Epoch: 27 | train_loss: 0.33634, val_loss: 0.52677, lr: 1.00E-05, _patience: 10\n",
      "Epoch: 28 | train_loss: 0.33180, val_loss: 0.52698, lr: 1.00E-05, _patience: 9\n",
      "Epoch: 29 | train_loss: 0.33010, val_loss: 0.52777, lr: 1.00E-05, _patience: 8\n",
      "Epoch: 30 | train_loss: 0.32884, val_loss: 0.52908, lr: 1.00E-05, _patience: 7\n",
      "Epoch: 31 | train_loss: 0.32769, val_loss: 0.52987, lr: 1.00E-06, _patience: 6\n",
      "Epoch: 32 | train_loss: 0.32447, val_loss: 0.52633, lr: 1.00E-06, _patience: 10\n",
      "Epoch: 33 | train_loss: 0.32347, val_loss: 0.52662, lr: 1.00E-06, _patience: 9\n",
      "Epoch: 34 | train_loss: 0.32332, val_loss: 0.52693, lr: 1.00E-06, _patience: 8\n",
      "Epoch: 35 | train_loss: 0.32308, val_loss: 0.52716, lr: 1.00E-06, _patience: 7\n",
      "Epoch: 36 | train_loss: 0.32302, val_loss: 0.52738, lr: 1.00E-07, _patience: 6\n",
      "Epoch: 37 | train_loss: 0.32263, val_loss: 0.52708, lr: 1.00E-07, _patience: 5\n",
      "Epoch: 38 | train_loss: 0.32248, val_loss: 0.52700, lr: 1.00E-07, _patience: 4\n",
      "Epoch: 39 | train_loss: 0.32227, val_loss: 0.52696, lr: 1.00E-07, _patience: 3\n",
      "Epoch: 40 | train_loss: 0.32221, val_loss: 0.52696, lr: 1.00E-08, _patience: 2\n",
      "Epoch: 41 | train_loss: 0.32273, val_loss: 0.52695, lr: 1.00E-08, _patience: 1\n",
      "Stopping early!\n"
     ]
    }
   ],
   "source": [
    "# Trainer module\n",
    "trainer = Trainer(\n",
    "    model=model, device=device, loss_fn=loss_fn,\n",
    "    optimizer=optimizer, scheduler=scheduler)\n",
    "\n",
    "# Train\n",
    "best_model = trainer.train(\n",
    "    NUM_EPOCHS, PATIENCE, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3434764d",
   "metadata": {},
   "source": [
    "## Đánh giá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fe9b78b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ccffe08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y_true, y_pred, classes):\n",
    "    \"\"\"Per-class performance metrics.\"\"\"\n",
    "    # Performance\n",
    "    performance = {\"overall\": {}, \"class\": {}}\n",
    "\n",
    "    # Overall performance\n",
    "    metrics = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "    performance[\"overall\"][\"precision\"] = metrics[0]\n",
    "    performance[\"overall\"][\"recall\"] = metrics[1]\n",
    "    performance[\"overall\"][\"f1\"] = metrics[2]\n",
    "    performance[\"overall\"][\"num_samples\"] = np.float64(len(y_true))\n",
    "\n",
    "    # Per-class performance\n",
    "    metrics = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
    "    for i in range(len(classes)):\n",
    "        performance[\"class\"][classes[i]] = {\n",
    "            \"precision\": metrics[0][i],\n",
    "            \"recall\": metrics[1][i],\n",
    "            \"f1\": metrics[2][i],\n",
    "            \"num_samples\": np.float64(metrics[3][i]),\n",
    "        }\n",
    "\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0129d6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp/ipykernel_16312/906435048.py:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  batch = np.array(batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"precision\": 0.814507964193054,\n",
      "  \"recall\": 0.8147222222222222,\n",
      "  \"f1\": 0.8144917556909449,\n",
      "  \"num_samples\": 18000.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Get predictions\n",
    "test_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)\n",
    "y_pred = np.argmax(y_prob, axis=1)\n",
    "\n",
    "# Determine performance\n",
    "performance = get_metrics(\n",
    "    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)\n",
    "print (json.dumps(performance[\"overall\"], indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
